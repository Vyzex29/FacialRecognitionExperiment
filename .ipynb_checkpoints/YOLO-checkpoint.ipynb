{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch/yolo install\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "!cd yolov5\n",
    "!pip install -r requirements.txt  # install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import torch, load model\n",
    "yolo versions - https://pytorch.org/hub/ultralytics_yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to C:\\Users\\User\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n",
      "YOLOv5  2022-4-8 torch 1.11.0 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f426e028a734bbaa467602ca89cbc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/14.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plot\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # yolo can do a video, which is cool, just passing instead of 0 a videofile is good\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    result = model(frame)\n",
    "    cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord ('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  YOLO Labeling images from collected images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Elon', 'id': 1}\n",
      "{'name': 'Eminem', 'id': 2}\n",
      "{'name': 'Lisa', 'id': 3}\n",
      "{'name': 'Valera', 'id': 4}\n",
      "[{'name': 'Elon', 'id': 1}, {'name': 'Eminem', 'id': 2}, {'name': 'Lisa', 'id': 3}, {'name': 'Valera', 'id': 4}]\n",
      "C:\\Users\\User\\PycharmProjects\\tensorFlowObjectDetection\\collectedimages\n",
      "C:\\Users\\User\\PycharmProjects\\tensorFlowObjectDetection\\TFRecognition\\YOLO\\YOLO_IMAGES\n"
     ]
    }
   ],
   "source": [
    "import PicturePipeline\n",
    "\n",
    "PicturePipeline.runPicturePipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Creating dataset.yaml\n",
    "Train yaml tutorial : https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:../TFRecognition/YOLO\n",
      "train: images\n",
      "val: images\n",
      "\n",
      "nc:4\n",
      "names:['Elon', 'Eminem', 'Lisa', 'Valera']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import Configuration.Config as config\n",
    "import TensorTrain as tt\n",
    "\n",
    "YOLO5_PATH = os.path.join(config.BASE_DIR, 'yolov5')\n",
    "dataset_file = os.path.join(YOLO5_PATH, 'dataset.yaml')\n",
    "labels = tt.GetLabels()\n",
    "file_name = \"classes.txt\"\n",
    "YOLO_LABELS_PATH = config.yolo['SAVE_LABELS_PATH']\n",
    "YOLO_LABELS_CLASSES = YOLO_LABELS_FILE = os.path.join(YOLO_LABELS_PATH, file_name)\n",
    "result = tt.file_to_list(YOLO_LABELS_CLASSES)\n",
    "\n",
    "yamlString = f\"path: {'../TFRecognition/YOLO'}\\n\" \\\n",
    "             f\"train: images\\n\" \\\n",
    "             f\"val: images\\n\" \\\n",
    "             f\"\\n\" \\\n",
    "             f\"nc: {len(labels)}\\n\" \\\n",
    "             f\"names: [\"\n",
    "\n",
    "for label in result:\n",
    "    yamlString += f\"'{label}', \"\n",
    "\n",
    "size = len(yamlString)\n",
    "mod_string = yamlString[:size - 2]\n",
    "mod_string += \"]\"\n",
    "print(mod_string)\n",
    "with open(dataset_file, \"w\") as f:\n",
    "    f.write(mod_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#   Training Yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# maybe try workers 2, with rtx3070 it seems a bit buggy\n",
    "!cd yolov5 && python train.py --img 800 --batch 16 --epochs 500 --data dataset.yaml --workers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#   Launching yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-4-8 torch 1.11.0 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "# force_reload = true if cache is not responding\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # yolo can do a video, which is cool, just passing instead of 0 a videofile is good\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    result = model(frame)\n",
    "    cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord ('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
